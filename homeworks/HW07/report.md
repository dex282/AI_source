# HW07 – Report

*Файл: `homeworks/HW07/report.md`*

*Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.*

## 1. Datasets

Вы были выбраны 3 датасета из 4: `S07-hw-dataset-01.csv`, `S07-hw-dataset-02.csv`, `S07-hw-dataset-04.csv`.

### 1.1 Dataset A

-   **Файл:** `S07-hw-dataset-01.csv`
-   **Размер:** (5000, 11)
-   **Признаки:** Все 10 признаков являются числовыми.
-   **Пропуски:** Нет.
-   **"Подлости" датасета:** Признаки находятся в разных масштабах, что критично для distance-based алгоритмов. Требуется обязательное масштабирование.

### 1.2 Dataset B

-   **Файл:** `S07-hw-dataset-02.csv`
-   **Размер:** (2000, 3)
-   **Признаки:** Все 2 признака являются числовыми.
-   **Пропуски:** Нет.
-   **"Подлости" датасета:** Данные имеют нелинейную структуру ("полумесяцы"), а также содержат выбросы. KMeans на таких данных, скорее всего, покажет плохой результат.

### 1.3 Dataset C

-   **Файл:** `S07-hw-dataset-04.csv`
-   **Размер:** (8000, 22)
-   **Признаки:** 19 числовых и 2 категориальных признака (`feature_cat_1`, `feature_cat_2`).
-   **Пропуски:** Есть, только в числовых признаках.
-   **"Подлости" датасета:** Высокая размерность, наличие пропусков и смешанный тип данных (числовые + категориальные). Требует комплексного конвейера предобработки.

## 2. Protocol

-   **Препроцессинг:** Для каждого датасета использовался `sklearn.pipeline.Pipeline`.
    -   **Dataset 1 & 2:** Применялся только `StandardScaler` для масштабирования числовых признаков.
    -   **Dataset 4:** Использовался `ColumnTransformer`, который применял два разных пайплайна:
        1.  Для числовых признаков: `SimpleImputer(strategy='median')` для заполнения пропусков, затем `StandardScaler`.
        2.  Для категориальных признаков: `SimpleImputer(strategy='most_frequent')`, затем `OneHotEncoder(handle_unknown="ignore")`.
-   **Поиск гиперпараметров:**
    -   **KMeans:** `k` перебиралось в диапазоне от 2 до 15.
    -   **DBSCAN:** `eps` перебирался в диапазоне от 0.1 до 0.5, `min_samples` — в диапазоне [3, 5, 7].
    -   **AgglomerativeClustering:** `linkage` сравнивался между 'ward' и 'complete' при фиксированном `k`.
    -   "Лучшая" модель выбиралась по максимальному значению `silhouette_score`.
-   **Метрики:** Качество оценивалось с помощью `silhouette_score`, `davies_bouldin_score` и `calinski_harabasz_score`.
    -   Для `silhouette_score` на больших датасетах (Dataset 1 и 4) использовался `sample_size`, чтобы избежать `MemoryError`.
    -   Для `DBSCAN` все метрики рассчитывались **только на точках, не являющихся шумом** (`labels != -1`), а доля шума выводилась отдельно.
-   **Визуализация:** Для лучшего решения по каждому датасету строился 2D scatter plot с помощью `PCA`.

## 3. Models

-   **Dataset 1:** Сравнивались `KMeans` (подбор `k`) и `AgglomerativeClustering` (подбор `k` и `linkage`).
-   **Dataset 2:** Сравнивались `KMeans` (подбор `k`) и `DBSCAN` (подбор `eps` и `min_samples`).
-   **Dataset 4:** Сравнивались `KMeans` (подбор `k`) и `AgglomerativeClustering` (подбор `k` и `linkage`).

Во всех запусках KMeans использовались `random_state=42` и `n_init=10`.

## 4. Results

### 4.1 Dataset A

-   **Лучший метод и параметры:** KMeans (k=9)
-   **Метрики (silhouette / DB / CH):** 0.287 / 1.309 / 1121.8
-   **Коротко:** После обязательного масштабирования KMeans смог выделить 9 достаточно компактных кластеров. Результат выглядит разумным, так как данные не имеют сложной нелинейной структуры.

### 4.2 Dataset B

-   **Лучший метод и параметры:** DBSCAN (eps=0.2, min_samples=7)
-   **Метрики (silhouette / DB / CH):** 0.513 / 0.563 / 2217.1
-   **Доля шума:** 1.55%
-   **Коротко:** DBSCAN ожидаемо показал себя лучше всего, так как он не ограничен предположением о сферической форме кластеров и смог корректно выделить два "полумесяца", отделив при этом шумовые точки.

### 4.3 Dataset C

-   **Лучший метод и параметры:** KMeans (k=15)
-   **Метрики (silhouette / DB / CH):** 0.126 / 1.947 / 1060.0
-   **Коротко:** На данных высокой размерности после комплексной предобработки (включая One-Hot Encoding) KMeans показал лучший результат. `AgglomerativeClustering` дал немного худший силуэт. Это говорит о том, что после всех преобразований в многомерном пространстве образовались относительно компактные, "шарообразные" группы.

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

-   **Где KMeans "ломается" и почему?**
    `KMeans` полностью провалился на **Dataset 2**. Он не способен распознавать кластеры произвольной формы, так как его задача — минимизировать расстояние до центроидов, что всегда приводит к поиску выпуклых, "шарообразных" кластеров. На данных в виде "полумесяцев" он просто делит их пополам.

-   **Где DBSCAN/иерархическая кластеризация выигрывают и почему?**
    `DBSCAN` показал свое полное преимущество на **Dataset 2**. Будучи основанным на плотности, он находит "связные" области точек, что позволяет ему идеально выделять кластеры сложной формы и игнорировать шумовые выбросы.

-   **Что сильнее всего влияло на результат?**
    1.  **Масштабирование (Dataset 1):** Критически важный шаг. Без него признаки с большим разбросом "заглушили" бы все остальные.
    2.  **Структура данных (Dataset 2):** Нелинейная форма кластеров была решающим фактором в пользу DBSCAN.
    3.  **Предобработка (Dataset 4):** Без комплексного пайплайна (импутация + кодирование + масштабирование) применить алгоритмы было бы невозможно.

### 5.2 Устойчивость (обязательно для одного датасета)

-   **Какую проверку устойчивости делали:**
    Проверка проводилась для `KMeans` (при k=9) на **Dataset 1**. Было выполнено 5 запусков с разными значениями `random_state` (0, 10, 20, 30, 40).
-   **Что получилось:**
    Схожесть всех парных разбиений оценивалась с помощью `Adjusted Rand Index (ARI)`. Среднее значение ARI составило **0.985**, а стандартное отклонение было очень низким.
-   **Вывод:**
    Результат **очень устойчив**. Значение ARI, близкое к 1.0, показывает, что независимо от случайной начальной инициализации центроидов, алгоритм практически всегда сходится к одному и тому же разбиению. Это говорит о наличии хорошо выраженной структуры в данных.

### 5.3 Интерпретация кластеров

-   **Как вы интерпретировали кластеры:**
    Интерпретация проводилась в основном визуально, на основе **PCA-графиков**. По взаимному расположению, форме и разделенности кластеров на 2D-плоскости делался вывод о качестве кластеризации.
-   **Выводы:**
    -   На **Dataset 1** кластеры выглядят как несколько пересекающихся, но различимых "облаков".
    -   На **Dataset 2** PCA-график для `DBSCAN` четко показывает два изогнутых кластера и несколько шумовых точек, подтверждая правильность выбора алгоритма.
    -   На **Dataset 4** кластеры сильно пересекаются в 2D-проекции, что ожидаемо для данных высокой размерности, но все же образуют различимые группы.

## 6. Conclusion

1.  **Препроцессинг — не опция, а необходимость.** Масштабирование, обработка пропусков и кодирование категорий — обязательные шаги перед применением distance-based алгоритмов.
2.  **"Нет бесплатного обеда".** Не существует универсального алгоритма кластеризации. Выбор всегда должен основываться на предварительном анализе данных и их предполагаемой структуре (`KMeans` для "шаров", `DBSCAN` для плотности и сложных форм).
3.  **Метрики — компас, а не конечная цель.** Внутренние метрики (silhouette и др.) помогают в подборе гиперпараметров, но их нельзя абсолютизировать. Визуальный анализ (PCA) и здравый смысл не менее важны (как в случае с Dataset 2, где формально у KMeans мог быть неплохой скор, но визуально результат был некорректен).
4.  **Протокол эксперимента важен.** Единый подход к предобработке и оценке позволяет честно сравнивать модели.
5.  **`MemoryError` — реальная проблема.** При расчете метрик на больших датасетах необходимо использовать `sample_size`, чтобы избежать переполнения памяти.
6.  **Устойчивость — важная проверка.** Она помогает убедиться, что найденное решение не является случайным артефактом, а отражает реальную структуру данных.

